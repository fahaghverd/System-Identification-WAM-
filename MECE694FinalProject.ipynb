{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MECE694FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pehzj_I5_qwu",
        "outputId": "d7f14041-6c0b-4e5b-e84d-33b190810b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXzyu9AWmeu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn import metrics\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA\n",
        "\n",
        "df = pd.read_json(\"/content/ID_data.json\")\n",
        "\n",
        "#df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "LD9ZDI9O_RIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using both q and dq as input features\n",
        "\n",
        "T = np.array([i for i in df['effort'].to_numpy()])\n",
        "q = np.array([i for i in df['position'].to_numpy()])\n",
        "dq = np.array([i for i in df['velocity'].to_numpy()])\n",
        "\n",
        "X = np.zeros((T.shape[0],6))\n",
        "X[:,0:3] = q[:,4:7]\n",
        "print(np.shape(X))\n",
        "X[:,3:6] = dq[:,4:7]\n",
        "Y = T[:,4:7]\n",
        "#print(dq[:,4:7])\n",
        "X = torch.from_numpy(X).float()\n",
        "Y = torch.from_numpy(Y).float()"
      ],
      "metadata": {
        "id": "SWDgKNpttEL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using only q as input feature\n",
        "\n",
        "T = np.array([i for i in df['effort'].to_numpy()])\n",
        "q = np.array([i for i in df['position'].to_numpy()])\n",
        "dq = np.array([i for i in df['velocity'].to_numpy()])\n",
        "\n",
        "#print(dq)\n",
        "\n",
        "X = np.zeros((T.shape[0],3))\n",
        "X[:,0:3] = q[:,4:7]\n",
        "Y = T[:,4:7]\n",
        "X = torch.from_numpy(X).float()\n",
        "Y = torch.from_numpy(Y).float()"
      ],
      "metadata": {
        "id": "NPHQxrPLuvvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using only dq as input feature\n",
        "\n",
        "T = np.array([i for i in df['effort'].to_numpy()])\n",
        "q = np.array([i for i in df['position'].to_numpy()])\n",
        "dq = np.array([i for i in df['velocity'].to_numpy()])\n",
        "\n",
        "#print(dq)\n",
        "\n",
        "X = np.zeros((T.shape[0],3))\n",
        "X[:,0:3] = dq[:,4:7]\n",
        "Y = T[:,4:7]\n",
        "X = torch.from_numpy(X).float()\n",
        "Y = torch.from_numpy(Y).float()"
      ],
      "metadata": {
        "id": "qQc_DZT5b1Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RKoQedqub00y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "loader = DataLoader(X, batch_size=len(X), num_workers=1)\n",
        "data = next(iter(loader))\n",
        "data.mean(), data.std()\n",
        "normalised_X = (X-data.mean())/data.std()\n",
        "print(normalised_X.mean(),normalised_X.std())\n"
      ],
      "metadata": {
        "id": "TPiEsXeqllZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data\n",
        "\n",
        "\n",
        "tr = 0.6\n",
        "vl = 0.25\n",
        "ts = 0.15\n",
        "\n",
        "n = T.shape[0]\n",
        "tr_n = round(tr*n)\n",
        "vl_n = round((n-tr_n)*(vl/(ts+vl)))\n",
        "ts_n = n-(tr_n+vl_n)\n",
        "\n",
        "X = normalised_X\n",
        "\n",
        "X_training = X[0:tr_n,:]\n",
        "Y_training = Y[0:tr_n,:] \n",
        "\n",
        "X_val =   X[tr_n:tr_n+vl_n,:]  \n",
        "Y_val =   Y[tr_n:tr_n+vl_n,:]  \n",
        "\n",
        "X_test =  X[tr_n+vl_n:-1,:] \n",
        "Y_test =  Y[tr_n+vl_n:-1,:] \n"
      ],
      "metadata": {
        "id": "7bJ2K2ektRxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using both dq and q as input feature\n",
        "#Defining the nn model\n",
        "net = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(6, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64,64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64,3)\n",
        ")\n",
        "\n",
        "\n",
        "#print(X_training.shape,Y_training.shape)\n",
        "\n",
        "#Define loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "#Defining the optimiser\n",
        "opt = optim.Adam(net.parameters(), lr=1e-5)\n",
        "#opt = optim.SGD(net.parameters(), lr=1e-3)\n",
        "\n",
        "dataset_train = data_utils.TensorDataset(X_training,Y_training)\n",
        "dataset_val = data_utils.TensorDataset(X_val,Y_val)\n",
        "\n",
        "N_samples = len(dataset_train)\n",
        "EpochMax = 1000\n",
        "batch_size = 32\n",
        "iter = math.ceil(N_samples/batch_size)\n",
        "\n",
        "#Seprating data to batches\n",
        "train_loader = DataLoader(dataset= dataset_train, batch_size=batch_size, shuffle=True,num_workers=1)\n",
        "val_loader = DataLoader(dataset= dataset_val, batch_size=1, shuffle=True,num_workers=1)\n",
        "\n",
        "loss = 0.0\n",
        "best_valid_loss = 1000.0\n",
        "PATH = \"state_dict_net.pt\"\n",
        "\n",
        "for epoch in range(EpochMax):\n",
        "  loss_batch = 0.0\n",
        "  print(f'Starting epoch {epoch+1}')\n",
        "  for b,(inputs, outputs) in enumerate(train_loader,0):\n",
        "    X_batch, Y_batch = (inputs, outputs)\n",
        "    \n",
        "    #Clear gradiants\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    #Forward pass\n",
        "    y_hat_batch = net(X_batch)\n",
        "    \n",
        "    loss_batch = loss_fn(y_hat_batch, Y_batch)\n",
        "    #print(loss_batch)\n",
        "    #Calculate gradians\n",
        "    loss_batch.backward()\n",
        "\n",
        "    #Update Weights\n",
        "    opt.step()\n",
        "   \n",
        "    loss += loss_batch\n",
        "  training_loss   = loss/iter\n",
        "  \n",
        "  #Validation Loop\n",
        "  loss_valid = 0.0\n",
        "  with torch.no_grad():\n",
        "    for X_val, Y_val in val_loader:\n",
        "      y_hat_val = net(X_val)\n",
        "      loss_valid = loss_fn(y_hat_val, Y_val)\n",
        "      if loss_valid<best_valid_loss:\n",
        "       best_valid_loss = loss_valid\n",
        "       torch.save(net.state_dict(),PATH)\n",
        "  loss = 0"
      ],
      "metadata": {
        "id": "xi-2rXeVJ8sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using only q or only dq as input feature\n",
        "#Defining the nn model\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(3, 64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64,64),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64,3)\n",
        ")\n",
        "\n",
        "\n",
        "#print(X_training.shape,Y_training.shape)\n",
        "\n",
        "#Define loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "#Defining the optimiser\n",
        "opt = optim.Adam(net.parameters(), lr=1e-5)\n",
        "#opt = optim.SGD(net.parameters(), lr=1e-3)\n",
        "\n",
        "dataset = data_utils.TensorDataset(X_training,Y_training)\n",
        "\n",
        "N_samples = len(dataset)\n",
        "EpochMax = 1000\n",
        "batch_size = 32\n",
        "iter = math.ceil(N_samples/batch_size)\n",
        "\n",
        "#Seprating data to batches\n",
        "train_loader = DataLoader(dataset= dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss = 0.0\n",
        "best_valid_loss = 1000.0\n",
        "PATH = \"state_dict_net.pt\"\n",
        "\n",
        "for epoch in range(EpochMax):\n",
        "  loss_batch = 0.0\n",
        "  for b,(inputs, outputs) in enumerate(train_loader,0):\n",
        "    X_batch, Y_batch = (inputs, outputs)\n",
        "    \n",
        "    #Clear gradiants\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    #Forward pass\n",
        "    y_hat_batch = net(X_batch)\n",
        "    \n",
        "    loss_batch = loss_fn(y_hat_batch, Y_batch)\n",
        "    print(loss_batch)\n",
        "    #Calculate gradians\n",
        "    loss_batch.backward()\n",
        "\n",
        "    #Update Ws\n",
        "    opt.step()\n",
        "   \n",
        "    loss += loss_batch\n",
        "  training_loss   = loss/iter\n",
        "  \n",
        "  #Validation Loop\n",
        "  loss_valid = 0.0\n",
        "  with torch.no_grad():\n",
        "    y_hat_val = net(X_val)\n",
        "    loss_valid = loss_fn(y_hat_val, Y_val)\n",
        "    if loss_valid<best_valid_loss:\n",
        "      best_valid_loss = loss_valid\n",
        "      torch.save(net.state_dict(),PATH)\n",
        "  loss = 0\n"
      ],
      "metadata": {
        "id": "aixxT4AXu7ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(actual, predicted):\n",
        "    return metrics.mean_squared_error(actual, predicted)\n",
        "\n",
        "def rmse(actual, predicted):\n",
        "    return np.sqrt(mse(actual, predicted))\n",
        "\n",
        "\n",
        "def nrmse(actual, predicted):\n",
        "    return rmse(actual, predicted) / (actual.max() - actual.min())"
      ],
      "metadata": {
        "id": "qv2rOUrY2Ms8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(PATH))\n",
        "y_hat_test = torch.tensor(net(X_test))\n",
        "\n",
        "#print(Y_test.shape,y_hat_test)\n",
        "y_hat_test.numpy()\n",
        "Y_test.numpy()\n",
        "\n",
        "print('test rmse',rmse(Y_test,y_hat_test))\n",
        "print('test nrmse',nrmse(Y_test,y_hat_test))\n",
        "\n",
        "y_hat_tr = torch.tensor(net(X_training))\n",
        "y_hat_tr.numpy()\n",
        "Y_test.numpy()\n",
        "\n",
        "print('training rmse',rmse(Y_training,y_hat_tr))\n",
        "print('nrmse',nrmse(Y_training,y_hat_tr))"
      ],
      "metadata": {
        "id": "CYxI33wendH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}